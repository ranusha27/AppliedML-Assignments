{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a4f41fc",
   "metadata": {},
   "source": [
    "# Applied Machine Learning - Assignment 1\n",
    "#### Submitted by \n",
    "- Anusha R\n",
    "- MDS202212\n",
    "- anushar@cmi.ac.in, r.anusha27@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c2cd6863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "860fea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "711392ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad0775f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access functions from prepare.ipynb\n",
    "\n",
    "%run prepare.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5fb42ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model on train data\n",
    "\n",
    "def train_model(model, vectorizer, train_df):\n",
    "    X_train = vectorizer.fit_transform(train_df['text'])\n",
    "    y_train = train_df['spam']\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', vectorizer),\n",
    "        ('model', model) ])\n",
    "\n",
    "    pipeline.fit(train_df['text'], y_train)\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "113c3fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score a model on given data\n",
    "\n",
    "def score_model(model, X_data, y_data):\n",
    "    y_pred = model.predict(X_data)\n",
    "    accuracy = accuracy_score(y_data, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e99dcaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model predictions\n",
    "\n",
    "def evaluate_model(model, X_data, y_data):\n",
    "    y_pred = model.predict(X_data)\n",
    "    report = classification_report(y_data, y_pred)\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85b7fa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune using training data\n",
    "\n",
    "def fine_tune_model(model, param_grid, X_train, y_train):\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcd8745",
   "metadata": {},
   "source": [
    "## Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9aeafb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "file_path = os.path.join(current_directory, 'Dataset/emails.csv')\n",
    "data = load_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c688e2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "\n",
    "preprocessed_data = preprocess_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "095b9d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data as train/validation/test\n",
    "\n",
    "train_data, validation_data, test_data = split_data(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6161064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "\n",
    "save_data(train_data, validation_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d89293",
   "metadata": {},
   "source": [
    "## Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bfd60ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Naive Bayes (MultinomialNB) model\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "nb_pipeline = train_model(nb_model, TfidfVectorizer(), train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26ae3231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.889697322467986\n",
      "Evaluation report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93      2616\n",
      "           1       1.00      0.54      0.70       820\n",
      "\n",
      "    accuracy                           0.89      3436\n",
      "   macro avg       0.94      0.77      0.82      3436\n",
      "weighted avg       0.90      0.89      0.88      3436\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# score and evaluation on train data\n",
    "\n",
    "print(\"Accuracy:\", score_model(nb_pipeline, train_data['text'], train_data['spam']))\n",
    "print(\"Evaluation report:\\n\", evaluate_model(nb_pipeline, train_data['text'], train_data['spam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09595c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.868237347294939\n",
      "Evaluation report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.92       872\n",
      "           1       1.00      0.45      0.62       274\n",
      "\n",
      "    accuracy                           0.87      1146\n",
      "   macro avg       0.93      0.72      0.77      1146\n",
      "weighted avg       0.89      0.87      0.85      1146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# score and evaluation on validation data\n",
    "\n",
    "print(\"Accuracy:\", score_model(nb_pipeline, validation_data['text'], validation_data['spam']))\n",
    "print(\"Evaluation report:\\n\", evaluate_model(nb_pipeline, validation_data['text'], validation_data['spam']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff6ba1d",
   "metadata": {},
   "source": [
    "### Hyperparameter -Tuning Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90596b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
       "                ('model', MultinomialNB(alpha=0.1))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune Naive Bayes model\n",
    "\n",
    "nb_param_grid = {'model__alpha': [0.1, 0.5, 1.0]}\n",
    "best_nb_model = fine_tune_model(nb_pipeline, nb_param_grid, train_data['text'], train_data['spam'])\n",
    "best_nb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51ec5dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Results:\n",
      "Training data Accuracy: 0.9988\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      2616\n",
      "           1       1.00      1.00      1.00       820\n",
      "\n",
      "    accuracy                           1.00      3436\n",
      "   macro avg       1.00      1.00      1.00      3436\n",
      "weighted avg       1.00      1.00      1.00      3436\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Score and evaluate Naive Bayes model on train set\n",
    "\n",
    "nb_train_accuracy = score_model(best_nb_model, train_data['text'], train_data['spam'])\n",
    "nb_train_report = evaluate_model(best_nb_model, train_data['text'], train_data['spam'])\n",
    "\n",
    "# Print Naive Bayes results\n",
    "print(\"Naive Bayes Results:\")\n",
    "print(f\"Training data Accuracy: {nb_train_accuracy:.4f}\")\n",
    "print(\"Classification Report:\\n\", nb_train_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "faceb823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Results:\n",
      "Validation Accuracy: 0.9817\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       872\n",
      "           1       1.00      0.92      0.96       274\n",
      "\n",
      "    accuracy                           0.98      1146\n",
      "   macro avg       0.99      0.96      0.97      1146\n",
      "weighted avg       0.98      0.98      0.98      1146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Score and evaluate Naive Bayes model on validation set\n",
    "\n",
    "nb_validation_accuracy = score_model(best_nb_model, validation_data['text'], validation_data['spam'])\n",
    "nb_validation_report = evaluate_model(best_nb_model, validation_data['text'], validation_data['spam'])\n",
    "\n",
    "# Print Naive Bayes results\n",
    "print(\"Naive Bayes Results:\")\n",
    "print(f\"Validation Accuracy: {nb_validation_accuracy:.4f}\")\n",
    "print(\"Classification Report:\\n\", nb_validation_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f919174d",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c96a07fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression model\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "lr_pipeline = train_model(lr_model, TfidfVectorizer(), train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36003c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9944703143189756\n",
      "Evaluation report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      2616\n",
      "           1       1.00      0.98      0.99       820\n",
      "\n",
      "    accuracy                           0.99      3436\n",
      "   macro avg       1.00      0.99      0.99      3436\n",
      "weighted avg       0.99      0.99      0.99      3436\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# score and evaluation on train data\n",
    "\n",
    "print(\"Accuracy:\", score_model(lr_pipeline, train_data['text'], train_data['spam']))\n",
    "print(\"Evaluation report:\\n\", evaluate_model(lr_pipeline, train_data['text'], train_data['spam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "348f59e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9825479930191972\n",
      "Evaluation report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       872\n",
      "           1       0.99      0.93      0.96       274\n",
      "\n",
      "    accuracy                           0.98      1146\n",
      "   macro avg       0.99      0.97      0.98      1146\n",
      "weighted avg       0.98      0.98      0.98      1146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# score and evaluation on validation data\n",
    "\n",
    "print(\"Accuracy:\", score_model(lr_pipeline, validation_data['text'], validation_data['spam']))\n",
    "print(\"Evaluation report:\\n\", evaluate_model(lr_pipeline, validation_data['text'], validation_data['spam']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527c301f",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning - Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e04db3aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
       "                ('model', LogisticRegression(C=10.0))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune Logistic Regression model\n",
    "\n",
    "lr_param_grid = {'model__C': [0.1, 1.0, 10.0]}\n",
    "best_lr_model = fine_tune_model(lr_pipeline, lr_param_grid, train_data['text'], train_data['spam'])\n",
    "best_lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd40f0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results:\n",
      "Training data Accuracy: 1.0000\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      2616\n",
      "           1       1.00      1.00      1.00       820\n",
      "\n",
      "    accuracy                           1.00      3436\n",
      "   macro avg       1.00      1.00      1.00      3436\n",
      "weighted avg       1.00      1.00      1.00      3436\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Score and evaluate Logistic Regression model on train set\n",
    "\n",
    "lr_train_accuracy = score_model(best_lr_model, train_data['text'], train_data['spam'])\n",
    "lr_train_report = evaluate_model(best_lr_model, train_data['text'], train_data['spam'])\n",
    "\n",
    "# Print Logistic Regression results\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(f\"Training data Accuracy: {lr_train_accuracy:.4f}\")\n",
    "print(\"Classification Report:\\n\", lr_train_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01f00e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results:\n",
      "Validation Accuracy: 0.9904\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       872\n",
      "           1       0.99      0.97      0.98       274\n",
      "\n",
      "    accuracy                           0.99      1146\n",
      "   macro avg       0.99      0.98      0.99      1146\n",
      "weighted avg       0.99      0.99      0.99      1146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Score and evaluate Logistic Regression model on validation set\n",
    "\n",
    "lr_validation_accuracy = score_model(best_lr_model, validation_data['text'], validation_data['spam'])\n",
    "lr_validation_report = evaluate_model(best_lr_model, validation_data['text'], validation_data['spam'])\n",
    "\n",
    "# Print Logistic Regression results\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(f\"Validation Accuracy: {lr_validation_accuracy:.4f}\")\n",
    "print(\"Classification Report:\\n\", lr_validation_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1770a1",
   "metadata": {},
   "source": [
    "##  Random Forest Classifier Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "67a0fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier()\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "rf_pipeline = train_model(rf_model, TfidfVectorizer(), train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8ff7124b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Evaluation report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      2616\n",
      "           1       1.00      1.00      1.00       820\n",
      "\n",
      "    accuracy                           1.00      3436\n",
      "   macro avg       1.00      1.00      1.00      3436\n",
      "weighted avg       1.00      1.00      1.00      3436\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# score and evaluation on train data\n",
    "\n",
    "print(\"Accuracy:\", score_model(rf_pipeline, train_data['text'], train_data['spam']))\n",
    "print(\"Evaluation report:\\n\", evaluate_model(rf_pipeline, train_data['text'], train_data['spam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f1e24df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9694589877835951\n",
      "Evaluation report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       872\n",
      "           1       0.99      0.88      0.93       274\n",
      "\n",
      "    accuracy                           0.97      1146\n",
      "   macro avg       0.98      0.94      0.96      1146\n",
      "weighted avg       0.97      0.97      0.97      1146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# score and evaluation on validation data\n",
    "\n",
    "print(\"Accuracy:\", score_model(rf_pipeline, validation_data['text'], validation_data['spam']))\n",
    "print(\"Evaluation report:\\n\", evaluate_model(rf_pipeline, validation_data['text'], validation_data['spam']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df66cd6e",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6f9866fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
       "                ('model',\n",
       "                 RandomForestClassifier(min_samples_split=5,\n",
       "                                        n_estimators=200))])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune the Random Forest Classifier\n",
    "\n",
    "rf_param_grid = {\n",
    "    'model__n_estimators': [50, 100, 200],\n",
    "    'model__max_depth': [None, 10, 20],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "best_rf_model = fine_tune_model(rf_pipeline, rf_param_grid, train_data['text'], train_data['spam'])\n",
    "best_rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d0df74c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results:\n",
      "Training data Accuracy: 0.9997\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      2616\n",
      "           1       1.00      1.00      1.00       820\n",
      "\n",
      "    accuracy                           1.00      3436\n",
      "   macro avg       1.00      1.00      1.00      3436\n",
      "weighted avg       1.00      1.00      1.00      3436\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Score and evaluate Random Forest model on train set\n",
    "\n",
    "rf_train_accuracy = score_model(best_rf_model, train_data['text'], train_data['spam'])\n",
    "rf_train_report = evaluate_model(best_rf_model, train_data['text'], train_data['spam'])\n",
    "\n",
    "# Print Random Forest results\n",
    "print(\"Random Forest Results:\")\n",
    "print(f\"Training data Accuracy: {rf_train_accuracy:.4f}\")\n",
    "print(\"Classification Report:\\n\", rf_train_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "422a4de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results:\n",
      "Validation Accuracy: 0.9634\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       872\n",
      "           1       0.99      0.85      0.92       274\n",
      "\n",
      "    accuracy                           0.96      1146\n",
      "   macro avg       0.97      0.93      0.95      1146\n",
      "weighted avg       0.96      0.96      0.96      1146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Score and evaluate Logistic Regression model on validation set\n",
    "\n",
    "rf_validation_accuracy = score_model(best_rf_model, validation_data['text'], validation_data['spam'])\n",
    "rf_validation_report = evaluate_model(best_rf_model, validation_data['text'], validation_data['spam'])\n",
    "\n",
    "# Print Random Forest results\n",
    "print(\"Random Forest Results:\")\n",
    "print(f\"Validation Accuracy: {rf_validation_accuracy:.4f}\")\n",
    "print(\"Classification Report:\\n\", rf_validation_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8332526d",
   "metadata": {},
   "source": [
    "## Predicting the results of test data and choosing the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c445ce2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_nb = best_nb_model.predict(test_data['text'])\n",
    "predictions_lr = best_lr_model.predict(test_data['text'])\n",
    "predictions_rf = best_rf_model.predict(test_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8e3ab951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store the metrics\n",
    "\n",
    "metrics_df = pd.DataFrame(index=['Naive Bayes', 'Logistic Regression', 'Random Forest'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "608a35eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and store accuracy, precision, recall, and F1 score for each model\n",
    "\n",
    "metrics_df['Accuracy'] = [accuracy_score(test_data['spam'], predictions_nb),\n",
    "                          accuracy_score(test_data['spam'], predictions_lr),\n",
    "                          accuracy_score(test_data['spam'], predictions_rf)]\n",
    "\n",
    "metrics_df['Precision'] = [precision_score(test_data['spam'], predictions_nb),\n",
    "                            precision_score(test_data['spam'], predictions_lr),\n",
    "                            precision_score(test_data['spam'], predictions_rf)]\n",
    "\n",
    "metrics_df['Recall'] = [recall_score(test_data['spam'], predictions_nb),\n",
    "                        recall_score(test_data['spam'], predictions_lr),\n",
    "                        recall_score(test_data['spam'], predictions_rf)]\n",
    "\n",
    "metrics_df['F1 Score'] = [f1_score(test_data['spam'], predictions_nb),\n",
    "                          f1_score(test_data['spam'], predictions_lr),\n",
    "                          f1_score(test_data['spam'], predictions_rf)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5eecc61b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Accuracy  Precision    Recall  F1 Score\n",
      "Naive Bayes          0.987784   0.996183  0.952555  0.973881\n",
      "Logistic Regression  0.993019   0.988971  0.981752  0.985348\n",
      "Random Forest        0.959860   0.987179  0.843066  0.909449\n"
     ]
    }
   ],
   "source": [
    "# Display the metrics table\n",
    "\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef617b32",
   "metadata": {},
   "source": [
    "The fine-tuned logistic regression model has the best accuracy, recall and f1-score values. It has a pretty good precision score too. Hence, I am choosing this model as the best one for spam email detection. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1e1227",
   "metadata": {},
   "source": [
    "## End of Assignment 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
